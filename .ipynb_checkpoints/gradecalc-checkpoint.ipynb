{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import numbers\n",
    "import copy\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsimplified Fraction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frac:\n",
    "    \n",
    "    def __init__(self, num, denom):\n",
    "        self.num = num\n",
    "        self.denom = denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grades manipulation functions (helpers for Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(grades, weights, biases = {}, drops = {}):\n",
    "    '''\n",
    "    grades -- dict mapping categories (eg. 'midterm') to grade lists\n",
    "    weights -- dict of category weights\n",
    "    '''\n",
    "    # Perform grade drops\n",
    "    grades = drop_grades(grades, drops)\n",
    "    \n",
    "    # Weighted sum of cat avgs\n",
    "    total = 0.0\n",
    "    # Sum of weights of used cats\n",
    "    cats_total = 0.0\n",
    "    for cat in grades.keys():\n",
    "        sigma = sum([x for (x,y) in grades[cat]])\n",
    "        if cat in biases:\n",
    "            sigma += biases[cat]\n",
    "        denom = sum([y for (x,y) in grades[cat]])\n",
    "        avg = sigma / denom\n",
    "        total += weights[cat] * avg\n",
    "        cats_total += weights[cat]\n",
    "\n",
    "    # If a category was never used, ignore it in the calculation\n",
    "    grade = total / cats_total\n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_grades(grades, drops):\n",
    "    undropped_grades = {}\n",
    "    for cat in grades.keys():\n",
    "        # Account for missing entry in drops\n",
    "        cat_drops = drops[cat] if cat in drops else 0\n",
    "        # Drops drops[cat] number of lowest grades for this category\n",
    "        cat_grades = grades[cat]\n",
    "        # Sort from most points lost to least points lost\n",
    "        # (Smaller assignments are handled correctly, since by points)\n",
    "        cat_grades.sort(key=lambda x : x[0] - x[1])\n",
    "        # Remove lowest grades, without out-of-bounds list\n",
    "        undropped_cat_grades = cat_grades[cat_drops:] if cat_drops < len(cat_grades) else []\n",
    "        undropped_grades[cat] = undropped_cat_grades\n",
    "    return undropped_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_grades(grades, future_grades):\n",
    "    '''\n",
    "    Add grade dicts together (union)\n",
    "    '''\n",
    "    all_grades = copy.deepcopy(grades)\n",
    "    for cat in future_grades.keys():\n",
    "        if cat in all_grades:\n",
    "            all_grades[cat] += future_grades[cat]\n",
    "        else:\n",
    "            all_grades[cat] = future_grades[cat]\n",
    "    return all_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Delete this\n",
    "def make_remaining_grades(grades, counts, value):\n",
    "    '''\n",
    "    Return grade dict that \"fills in\" grades with value (so that they total to counts)\n",
    "    '''\n",
    "    remaining_grades = {}\n",
    "    for cat in counts.keys():\n",
    "        cat_grades = grades[cat] if cat in grades else []\n",
    "        remaining_grades[cat] = [value]*(counts[cat]-len(cat_grades))\n",
    "    return remaining_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_grades(grades, percent):\n",
    "    '''\n",
    "    Returns grade dict with points earned for each grade set to percent times points total\n",
    "    '''\n",
    "    return {cat: [(percent*y, y) for (x,y) in grades[cat]] for cat in grades.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "b = ['c','d']\n",
    "x = list(a.keys())\n",
    "x.extend(b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove biases\n",
    "\n",
    "class Course:\n",
    "    '''\n",
    "    Contains all the info needed for calculating grade for one course.\n",
    "    '''\n",
    "    EXPECTED_FUTURE_GRADE = 0.90\n",
    "    \n",
    "    def __init__(self, weights, grades, future_grades = {}, biases = {}, drops = {}):\n",
    "        '''\n",
    "        weights -- dict mapping categories (eg. 'midterm') to percentage weights\n",
    "        grades -- dict mapping cats to lists o' grades (tuples, (points earned, points total))\n",
    "        future_grades -- dict mapping cats to lists o' predicted future grade\n",
    "        biases -- dict mapping cats to \"extra credit assignment\"\n",
    "        drops -- dict mapping cats to number of dropped assignments\n",
    "        counts -- dict mapping cats to number of total assignments\n",
    "        '''\n",
    "        self.weights = weights\n",
    "        # Check that no extra categories are defined (important for typos)\n",
    "        keys_to_verify = list(grades.keys()) + list(future_grades.keys()) + list(biases.keys()) + list(drops.keys())\n",
    "        for cat in keys_to_verify:\n",
    "            if not cat in self.weights.keys():\n",
    "                raise Exception(\"Grade category '{}' is not listed in weights.\")\n",
    "        # Convert any integers to tuples in grades dict\n",
    "        for cat in grades.keys():\n",
    "            grades[cat] = [(x if isinstance(x, tuple) else (x,1)) for x in grades[cat]]\n",
    "        for cat in future_grades.keys():\n",
    "            future_grades[cat] = [(x if isinstance(x, tuple) else (0,x)) for x in future_grades[cat]]\n",
    "        self.grades = grades\n",
    "        self.future_grades = future_grades\n",
    "        self.biases = biases\n",
    "        self.drops = drops\n",
    "\n",
    "    def percent_complete(self):\n",
    "        # Fill in remaining assignments with 0%, and past assignments with 100%\n",
    "        all_grades = add_grades(fill_grades(self.grades, 1), fill_grades(self.future_grades, 0))\n",
    "        return calculate_grade(all_grades, self.weights, biases = self.biases, drops = {})\n",
    "        \n",
    "    def current_grade(self):\n",
    "        '''\n",
    "        Calculate grade, ignoring missing categories\n",
    "        '''\n",
    "        # Redistribute weights so that large sections with few entries dont \"overcount\"\n",
    "        redistributed_weights = {}\n",
    "        for cat in self.weights.keys():\n",
    "            cat_grades = self.grades[cat] if cat in self.grades else []\n",
    "            redistributed_weights[cat] = self.weights[cat] * len(cat_grades)\n",
    "        return calculate_grade(self.grades, redistributed_weights, biases = self.biases, drops = self.drops)\n",
    "\n",
    "    def projected_grade(self):\n",
    "        '''\n",
    "        Calculate grade, given projected future grades\n",
    "        future_grades -- int or dict mapping categories to lists o' grades\n",
    "        '''\n",
    "        # Make the remaining grades future_grades\n",
    "        proj_grades = fill_grades(self.future_grades, self.EXPECTED_FUTURE_GRADE)\n",
    "        all_grades = add_grades(self.grades, proj_grades)\n",
    "        \n",
    "        return calculate_grade(all_grades, self.weights, biases = self.biases, drops = self.drops)\n",
    "    \n",
    "    def raw_grade(self):\n",
    "        '''\n",
    "        Calculate grade, with future grades set to zero (aka percent done with course)\n",
    "        '''\n",
    "        # Fill in remaining assignments with 0s\n",
    "        zero_grades = fill_grades(self.future_grades, 0)\n",
    "        undropped_grades = drop_grades(self.grades, self.drops)\n",
    "        all_grades = add_grades(undropped_grades, zero_grades)\n",
    "        return calculate_grade(all_grades, self.weights, biases = self.biases, drops = {})\n",
    "\n",
    "    \n",
    "    def display(self):\n",
    "        string = \"Course Analysis: \\n\"\n",
    "        string += \"Current Grade: \\t\\t{:.2f}\\n\".format(self.current_grade())\n",
    "        pgrade = self.projected_grade()\n",
    "        string += (\"Projected Grade: \\t{:.2f} \\t(expecting {} for future grades)\\n\"\n",
    "                    .format(pgrade, self.EXPECTED_FUTURE_GRADE))\n",
    "        string += \"Percent Complete: \\t{:.2f}\\n\".format(self.percent_complete())\n",
    "#        string += \"Raw Grade: \\t\\t{:.2f} \\t(percent of total course points)\".format(self.raw_grade())\n",
    "        return string\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.display()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Grade Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Course Analysis: \n",
       "Current Grade: \t\t0.98\n",
       "Projected Grade: \t0.97 \t(expecting 0.95 for future grades)\n",
       "Percent Complete: \t0.70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_180 = Course(\n",
    "    weights = {'homework':0.5, 'midterm':0.2, 'final':0.3},\n",
    "    grades = {\n",
    "        'homework': [(67,70), (67,70), (67,70), (70,70), (70,70), (70,70), (70,70)],\n",
    "        'midterm': [(95,100)]\n",
    "    },\n",
    "    drops = {'homework':1},\n",
    "    future_grades = {\n",
    "        'final':[100]\n",
    "    }\n",
    ")\n",
    "\n",
    "math_180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Course Analysis: \n",
       "Current Grade: \t\t0.94\n",
       "Projected Grade: \t0.97 \t(expecting 0.95 for future grades)\n",
       "Percent Complete: \t0.70"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_135 = Course(\n",
    "    weights = {'homework':0.2, 'midterm':0.5, 'final':0.3},\n",
    "    grades = {\n",
    "        'homework': [(3,3), (19,20), (0,20), (20,20), (18,20), (13,20), (19,20), (19,20), (0,20)],\n",
    "        'midterm': [(40,40), (40,40)]\n",
    "    },\n",
    "    drops = {'homework':2},\n",
    "    future_grades = {\n",
    "        'homework': [],\n",
    "        'final': [100],\n",
    "    }\n",
    ")\n",
    "math_135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Course Analysis: \n",
       "Current Grade: \t\t1.00\n",
       "Projected Grade: \t0.97 \t(expecting 0.95 for future grades)\n",
       "Percent Complete: \t0.45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_151a = Course(\n",
    "    weights = {'homework': 0.2, 'midterm':0.3, 'final':0.5},\n",
    "    grades = {\n",
    "        'homework': [1, 0.99, 1],\n",
    "        'midterm': [1],\n",
    "    },\n",
    "    drops = {'homework':1},\n",
    "    future_grades= {\n",
    "        'homework': [1],\n",
    "        'final': [1]\n",
    "    }\n",
    ")\n",
    "math_151a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Course Analysis: \n",
       "Current Grade: \t\t1.03\n",
       "Projected Grade: \t1.02 \t(expecting 0.95 for future grades)\n",
       "Percent Complete: \t1.00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_33 = Course(\n",
    "    weights = {'homework': 0.05, 'lab': 0.4, 'midterm':0.25, 'final':0.3},\n",
    "    grades = {\n",
    "        'homework': [(1,1), (1,1), (1,1), (1,1),(1,1)],\n",
    "        'lab': [(0,1), (100,100), (100,100), (95,100), (114,100)],\n",
    "        'midterm': [(100,100)],\n",
    "        'final': [(100,100)],\n",
    "    },\n",
    "    drops = {\n",
    "        'lab': 1,\n",
    "    },\n",
    "    future_grades={\n",
    "    }\n",
    ")\n",
    "cs_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Course Analysis: \n",
       "Current Grade: \t\t1.00\n",
       "Projected Grade: \t0.98 \t(expecting 0.95 for future grades)\n",
       "Percent Complete: \t0.67"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_131bh = Course(\n",
    "    weights = {'homework': 0.1, 'quiz':0.2, 'midterm':0.4, 'final':0.35},\n",
    "    grades = {\n",
    "        'homework': [(10,10), (10,10), (10,10), (10,10), (10,10)],\n",
    "        'quiz': [(10,10), (10,10), (10,10), (10,10),(10,10)],\n",
    "        'midterm': [(28,28), (28,28)],\n",
    "    },\n",
    "    future_grades= {\n",
    "        'final': [100]\n",
    "    }\n",
    ")\n",
    "\n",
    "math_131bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
